## Weight Initialization
* 가중치는 모델 파라미터에서 가장 큰 비중을 차지하기에 초기화 방식에 따라 성능의 차이가 생길 수 밖에 없다.

![image](https://user-images.githubusercontent.com/83739271/209782151-7586d809-cc1e-49fe-b0b6-fd4cf6752722.png)

* 가중치를 0으로 초기화 한 경우
  * 뉴런의 가중치가 0이면 당연히 합산 결과 또한 0이고, 활성함수는 0을 입력 받아서 같은 값만 출력한다. 이 출력값들은 의미가 없고 역전파도 이뤄지지 않아서 학습이 진행되지 않게 됩니다.
  * ReLU의 출력: 0
  * 하이퍼볼릭 탄젠트 출력: 0
  * 시그모이드 출력: 0
  * 소프트맥스 : 모든 클래스의 확률이 동일해지니까 균등 분포를 출력한다

* 가중치를 상수로 초기화
  * 은닉 뉴런는 입력 값과 가중치 값이 같아서 가중 합산의 결과와 활성 함수의 실행 결과도 같게 된다. 그로 인해 뉴런의 갯수는 많더라도 신경망에 대칭성이 생겨서 실제로는 하나의 뉴런만 있는 것 같은 성능저하 효과를 불러일으킵니다.

* 가중치를 아주 작은 난수로 초기화
  * 모델 가중치를 가우시안 분포 N(0,0.01)을 따르는 난수로 초기화 시, 계층이 깊어질수록 출력이 점점 0으로 수렴하게 됩니다.
  * 가중치가 너무 작으면 뉴런의 출력이 작아지고 그로 인해서 입력 데이터가 여러 계층을 통과를 하면 할수록 0에 가깝게 변하게 되고, 이렇게 되면 가중 합산이 0이 되어서 가중치를 0으로 초기화하는 것과 같아지기에 의미있는 출력이 나오지도 못하고 학습 또한 진행이 되지 않습니다.

* 가중치를 큰 난수로 초기화
  * 반대로 가중치를 엄청 크게 키워서 가우시안 분포 N(0, 1)을 따른다면, 입력 데이터가 각 계층을 통과하면서 -1이나 1로 수렴하는 현상이 일어나고 이로 인해서 그래디언트가 0으로 포화되는 현상이 벌어져서 그래디언트가 소실이 생기게 되고 그로 인해 학습이 중단됩니다.

이 외에 다른 가중치 초기화 방법
* Xavier 초기화
* He 초기화
